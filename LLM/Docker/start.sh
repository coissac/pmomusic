#!/bin/bash
set -e

# Lancer Ollama en arriÃ¨re-plan
# export OLLAMA_MODELS=/models
# echo "ðŸ”¹ DÃ©marrage de Ollama..." 1>&2
# ollama serve 2>&1 \
# | grep -vF "decode: cannot decode batches with this context (use llama_encode() instead)" \
# | sed 's/^/ ðŸ”¹[Ollama server] /' 1>&2 &

# sleep 10

if [[ -n "$1" ]] ; then 
    eval $*
fi

echo "ðŸ”¹ Preaload Ollama models: "
ollama ls  | sed 's/^/ ðŸ”¹ /' 1>&2

# VÃ©rifier / prÃ©charger le modÃ¨le d'embedding

echo "ðŸ”¹ VÃ©rification du modÃ¨le d'embedding: $EMBED_MODEL" 1>&2
if ! ollama list | grep -q "$EMBED_MODEL"; then
    echo " ðŸ”¹ ModÃ¨le $EMBED_MODEL non trouvÃ©, tÃ©lÃ©chargement..." 1>&2
    ollama pull "$EMBED_MODEL"
else
    echo " ðŸ”¹ ModÃ¨le $EMBED_MODEL dÃ©jÃ  prÃ©sent" 1>&2
fi

echo "ðŸ”¹ VÃ©rification du modÃ¨le LLM: $OLLAMA_MODEL" 1>&2
if ! ollama list | grep -q "$OLLAMA_MODEL"; then
    echo " ðŸ”¹ ModÃ¨le $OLLAMA_MODEL non trouvÃ©, tÃ©lÃ©chargement..." 1>&2
    ollama pull "$OLLAMA_MODEL"
else
    echo " ðŸ”¹ ModÃ¨le $OLLAMA_MODEL dÃ©jÃ  prÃ©sent" 1>&2
fi


# Lancer FastAPI
echo "ðŸ”¹ DÃ©marrage de FastAPI..." 1>&2
exec uvicorn app:app --host 0.0.0.0 --port 8000 --reload
